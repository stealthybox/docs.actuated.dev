{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"actuated \u00b6 Actuated brings blazingly fast, secure builds to self-hosted CI runners. Building containers on self-hosted runners is slow and insecure \u00b6 Most solutions that use containers for running Docker or Kubernetes in CI have very poor security boundaries. They require either privileged containers (root on the host), a shared Docker socket (root on the host), third-party tools which don't integrate well and still require root to mount folders, or user namespaces which come with their own limitations. The chances are, if you use Docker or K8s in your CI, and run with: actions-controller-runtime , Jenkins, or GitLab, then you may be compromising on security or user experience. Management is a nightmare \u00b6 Self-hosted CI runners are continually out of date, and require fine-tuning to get all the right packages in place and Kernel modules to build containers and cloud-native software. You'll also have to spend extra time making sure builds don't conflict, and that they can't cause side effects to system-level packages. What if you need two different version of some software? If you haven't felt this pain yet, then perhaps you're blissfully unaware or are not updating your packages? Are you running privileged containers for CI in your organisation? Are you sharing a Docker Socket (just as bad!)? Are you running Docker in Docker (DIND) ? \ud83d\ude48 Self-managed runners are inefficient and overprovisioned \u00b6 Self-hosted runners are typically over-provisioned meaning you're spending too much money. Why are they over-provisioned? Because you never know how many jobs you'll have to run, so you have to make them bigger, or have too many hosts available. Why are they inefficient? By default, the self-hosted runner will only schedule one job per host at a time, because GitHub has no knowledge of the capacity of your machines. So each and every build you run could consume all the resources on the host. The second reason is that builds often conflict with one another causing side effects that only happen in CI and are really difficult to track down and reproduce. Actuated uses VMs to slice up the whole machine, and can run many builds in parallel. The net effect is that your build queue will get cleared down much more quickly. Hands-free, VM-level isolation \u00b6 Actuated provides a fast-booting microVM which can run Docker, Kubernetes and anything else you need, with full root on the VM, and no access to the host. Each environment is created just in time to take a build, and is removed immediately after. Boot time is usually ~1-2 seconds for the VM, that extra second is because we start Docker as part of the boot-up process. What does \"actuated\" mean? Something that activates or impels itself; specifically (a machine, device, etc.) that causes itself to begin operating automatically, self-activating. We maintain a VM image that is updated regularly through an automated build, so you don't have to install SDKs, runtimes or language packs on your build machines. Just enable automated updates on your server then install the actuated agent. We'll do the rest including managing efficient allocation across your fleet of servers, and updating the CI image. And actuated will run your jobs efficiently across a fleet of hosts, or a single machine. They each need to be either bare-metal hosts (think: AWS Metal / Graviton, Equinix Metal, etc), or support nested virtualization (a feature available on GCP and DigitalOcean) Conceptual overview Actuated will schedule builds across your fleet of agents, packing them in densely, without overloading the host. Each microVM will run just one build before being destroyed to ensure a clean, isolated build. Learn more in the FAQ Watch a live demo \u00b6 Alex shows you how actuated uses an isolated, immutable microVM to run K3s inside of a GitHub Action, followed by a matrix build that causes 5 VMs to be launched. You'll see how quick and easy it is to enable actuated, and how it can buffer and queue up jobs, when there's no remaining capacity in your fleet of agents. You can also watch a webinar that Alex recorded with Richard Case from Weaveworks on how microVMs compare to containers and legacy VMs, you'll see Alex's demo at: 1:13:19 . Get started \u00b6 Register for the pilot Read the FAQ Enable actuated for an existing repository Got questions, comments or suggestions? \u00b6 actuated is trademark of OpenFaaS Ltd. You can contact the team working on actuated via email at: contact@openfaas.com Follow @selfactuated on Twitter for updates and announcements","title":"Introduction"},{"location":"#actuated","text":"Actuated brings blazingly fast, secure builds to self-hosted CI runners.","title":"actuated"},{"location":"#building-containers-on-self-hosted-runners-is-slow-and-insecure","text":"Most solutions that use containers for running Docker or Kubernetes in CI have very poor security boundaries. They require either privileged containers (root on the host), a shared Docker socket (root on the host), third-party tools which don't integrate well and still require root to mount folders, or user namespaces which come with their own limitations. The chances are, if you use Docker or K8s in your CI, and run with: actions-controller-runtime , Jenkins, or GitLab, then you may be compromising on security or user experience.","title":"Building containers on self-hosted runners is slow and insecure"},{"location":"#management-is-a-nightmare","text":"Self-hosted CI runners are continually out of date, and require fine-tuning to get all the right packages in place and Kernel modules to build containers and cloud-native software. You'll also have to spend extra time making sure builds don't conflict, and that they can't cause side effects to system-level packages. What if you need two different version of some software? If you haven't felt this pain yet, then perhaps you're blissfully unaware or are not updating your packages? Are you running privileged containers for CI in your organisation? Are you sharing a Docker Socket (just as bad!)? Are you running Docker in Docker (DIND) ? \ud83d\ude48","title":"Management is a nightmare"},{"location":"#self-managed-runners-are-inefficient-and-overprovisioned","text":"Self-hosted runners are typically over-provisioned meaning you're spending too much money. Why are they over-provisioned? Because you never know how many jobs you'll have to run, so you have to make them bigger, or have too many hosts available. Why are they inefficient? By default, the self-hosted runner will only schedule one job per host at a time, because GitHub has no knowledge of the capacity of your machines. So each and every build you run could consume all the resources on the host. The second reason is that builds often conflict with one another causing side effects that only happen in CI and are really difficult to track down and reproduce. Actuated uses VMs to slice up the whole machine, and can run many builds in parallel. The net effect is that your build queue will get cleared down much more quickly.","title":"Self-managed runners are inefficient and overprovisioned"},{"location":"#hands-free-vm-level-isolation","text":"Actuated provides a fast-booting microVM which can run Docker, Kubernetes and anything else you need, with full root on the VM, and no access to the host. Each environment is created just in time to take a build, and is removed immediately after. Boot time is usually ~1-2 seconds for the VM, that extra second is because we start Docker as part of the boot-up process. What does \"actuated\" mean? Something that activates or impels itself; specifically (a machine, device, etc.) that causes itself to begin operating automatically, self-activating. We maintain a VM image that is updated regularly through an automated build, so you don't have to install SDKs, runtimes or language packs on your build machines. Just enable automated updates on your server then install the actuated agent. We'll do the rest including managing efficient allocation across your fleet of servers, and updating the CI image. And actuated will run your jobs efficiently across a fleet of hosts, or a single machine. They each need to be either bare-metal hosts (think: AWS Metal / Graviton, Equinix Metal, etc), or support nested virtualization (a feature available on GCP and DigitalOcean) Conceptual overview Actuated will schedule builds across your fleet of agents, packing them in densely, without overloading the host. Each microVM will run just one build before being destroyed to ensure a clean, isolated build. Learn more in the FAQ","title":"Hands-free, VM-level isolation"},{"location":"#watch-a-live-demo","text":"Alex shows you how actuated uses an isolated, immutable microVM to run K3s inside of a GitHub Action, followed by a matrix build that causes 5 VMs to be launched. You'll see how quick and easy it is to enable actuated, and how it can buffer and queue up jobs, when there's no remaining capacity in your fleet of agents. You can also watch a webinar that Alex recorded with Richard Case from Weaveworks on how microVMs compare to containers and legacy VMs, you'll see Alex's demo at: 1:13:19 .","title":"Watch a live demo"},{"location":"#get-started","text":"Register for the pilot Read the FAQ Enable actuated for an existing repository","title":"Get started"},{"location":"#got-questions-comments-or-suggestions","text":"actuated is trademark of OpenFaaS Ltd. You can contact the team working on actuated via email at: contact@openfaas.com Follow @selfactuated on Twitter for updates and announcements","title":"Got questions, comments or suggestions?"},{"location":"add-agent/","text":"Add your first agent to actuated \u00b6 actuated is split into three parts: An agent that you run on your own machines or VMs, which can launch a VM with a single-use GitHub Actions runner A VM image launched by the agent, with all the preinstalled software found on a hosted GitHub Actions runner Our own control plane that talks to GitHub on your behalf, and schedules builds across your fleet of agents. We look after 2 and 3 which means you just have to set up one or more agent to get started. Have you registered your organisation yet? Before you can add an agent, you or your GitHub organisation admin will need to install the: Actuated GitHub App . Decide where to run your agent \u00b6 There are three places you can run an agent: Bare-metal on-premises (cheap, convenient, high performance) This could be a machine racked in your server room, under your desk, or in a co-lo somewhere. It can be a cheap and convenient way to re-use existing hardware. Make sure you segment or isolate the agent into its own subnet, VLAN, DMZ, or VPC so that it cannot access the rest of your network. If you are thinking of running an actuated runner at home, we can share some iptables rules that worked well for our own testing. If you need to build for ARM64, we've shown that a Raspberry Pi 4 is faster than emulating ARM on a GitHub Hosted Runner Bare-metal on the cloud (higher cost, convenient, high performance) You can provision bare-metal hosts in the cloud using any number of providers like AWS, Alibaba Cloud, Cherry Servers, Equinix Metal, FastHosts, OVHcloud, Scaleway and Vultr, see a list here For Intel/AMD builds on AWS, you'll need to use AWS i3.metal . For ARM64 builds on AWS, the a1.metal is ideal. If you're not using AWS, or want larger machines, Equinix Metal offer bare-metal as a service for both Intel/AMD and ARM64 machines. This option is both convenient and offers the highest performance available, however bare-metal machines tends to be priced higher than you may be used to with VMs. Bear in mind that you may be able to run a single, larger bare-metal machine where you used to need half a dozen cloud VMs, since actuated can schedule builds much more efficiently than the built-in self-hosted runner from GitHub. Cloud Virtual Machines (VMs) that support nested virtualization (lowest cost, convenient, mid-level performance) Both DigitalOcean and Google Compute Platform (GCP) (new customers get 300 USD free credits from GCP) support nested virtualisation on their Virtual Machines (VMs). This option may not have the raw speed and throughput of a dedicated, bare-metal host, but keeps costs low and is convenient for getting started. The recommended Operating System for an Actuated Agent is: Ubuntu Server 22.04 or Ubuntu Server 20.04. Review the End User License Agreement (EULA) \u00b6 Make sure you've read the Actuated EULA before registering your organisation with the actuated GitHub App, or starting the agent binary on one of your hosts. Set up your first agent \u00b6 Download the agent and installation script Once you've decided where to set up your first agent, you'll need to download the installation package from a container registry Install crane : curl -sLS https://get.arkade.dev | sudo sh arkade get crane sudo mv crane /usr/local/bin/ rm -rf agent mkdir -p agent crane export ghcr.io/openfaasltd/actuated-agent:latest | tar -xvf - -C ./agent Install the agent binary to /usr/local/bin/ : sudo mv ./agent/agent* /usr/local/bin/ Run the setup.sh script which will install all the required dependencies like containerd, CNI and Firecracker. cd agent sudo ./install.sh Create a file to store your license from Gumroad: mkdir -p ~/.actuated # Paste the contents, hit enter, then Control + D # Or edit the file with nano/vim cat > $HOME /.actuated/LICENSE Generate an RSA keypair cd ~/.actuated/ agent keygen The RSA keypair is only used to encrypt messages and cannot. RSA keys are sometimes used with SSH sessions, however actuated does not use any form of SSH at this time. This will write: key_rsa and key_rsa.pub to the current working folder. Keep the key_rsa private, we will not ask you to share this file with us. Share key_rsa.pub with us via email or Slack. This key is not confidential, so don't worry about sharing it. cat ~/.actuated/key_rsa.pub Install the agent's authentication token. Create an API token for us to present when we send jobs to your Actuated Agent: openssl rand -base64 32 > ~/.actuated/TOKEN Encrypt the token with our public key and email .actuated/TOKEN.ENC to us, or share it with us on Slack: cat <<EOF > actuated.pem -----BEGIN RSA PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAo9EC7IrP8zTE9jm8agPa m0D/sFfmAlchhskLZksO4ZYzDHK9fuQ9oEhPYVkgrU5TifbL5UchdsSn//ELSy2Q TPRQoXVMdzPgLCrn15U+Xr7KpV3iNBV1go+ZzNE/ymdyS2kCCjxYiBLVuymn20hA ZzqkHSyOeM6IrG+A462KfmN0vqIpubpMkoK/wSkSSDjN0SoMWc9gaAqEFEHkSt9+ t65fIdzG0sKSEMb613WG+K/A/WBrcdqGHWoMG2h2CpK12tNobZEt3yCL0WVgkAKU VwaHniNYHn5niJHH/DgvXMWDECoKA1ZJyMdWC3MuIlyfWVzT5N7a/HPTzyzlrdCl bwIDAQAB -----END RSA PUBLIC KEY----- EOF agent encrypt --key ./actuated.pem \\ --in $HOME /.actuated/TOKEN \\ --out $HOME /.actuated/TOKEN.enc Post-pilot, we will provide a more automated way to exchange this token. Add HTTPS for the agent's endpoint The actuated control plane will only communicate with a HTTPS endpoint to ensure properly encryption is in place. An API token is used in addition with the TLS connection for all requests. In addition, any bootstrap tokens sent to the agent are further encrypted with the agent's public key. For hosts with public IPs, you will need to use the built-in TLS provisioning with Let's Encrypt. For hosts behind a firewall, NAT or in a private datacenter, you can use inlets to create a secure tunnel to the agent. We're considering other models for after the pilot, for instance GitHub's own API has the runner make an outbound connection and uses long-polling. See also: expose the agent with HTTPS Start the agent For an Intel/AMD Actuated Agent, create a start.sh file: #!/bin/bash echo Running Agent from: ./agent DOMAIN = agent1.example.com sudo -E agent up \\ --image-ref = ghcr.io/openfaasltd/actuated-ubuntu20.04:x86-64-dfb3ac12ae8d41ba00d5264e988256ce89acc9c6 \\ --kernel-ref = ghcr.io/openfaasltd/actuated-kernel-5.10.77:x86-64-dfb3ac12ae8d41ba00d5264e988256ce89acc9c6 \\ --letsencrypt-domain $DOMAIN \\ --letsencrypt-email webmaster@ $DOMAIN For an Actuated Agent behind an inlets tunnel : #!/bin/bash echo Running Agent from: ./agent sudo -E agent up \\ --image-ref = ghcr.io/openfaasltd/actuated-ubuntu20.04:aarch64-dfb3ac12ae8d41ba00d5264e988256ce89acc9c6 \\ --kernel-ref = ghcr.io/openfaasltd/actuated-kernel-5.10.77:aarch64-dfb3ac12ae8d41ba00d5264e988256ce89acc9c6 \\ --listen-addr 127 .0.0.1: For ARM64 Actuated Agents, change the prefix of the image tags from x86-64- to aarch64- You can also run the Actuated Agent software as a systemd unit file for automatic restarts and to start upon boot-up. Next steps \u00b6 You can now start your first build and see it run on your actuated agent. Start a build on your agent See also: Troubleshooting your agent","title":"Add your first agent"},{"location":"add-agent/#add-your-first-agent-to-actuated","text":"actuated is split into three parts: An agent that you run on your own machines or VMs, which can launch a VM with a single-use GitHub Actions runner A VM image launched by the agent, with all the preinstalled software found on a hosted GitHub Actions runner Our own control plane that talks to GitHub on your behalf, and schedules builds across your fleet of agents. We look after 2 and 3 which means you just have to set up one or more agent to get started. Have you registered your organisation yet? Before you can add an agent, you or your GitHub organisation admin will need to install the: Actuated GitHub App .","title":"Add your first agent to actuated"},{"location":"add-agent/#decide-where-to-run-your-agent","text":"There are three places you can run an agent: Bare-metal on-premises (cheap, convenient, high performance) This could be a machine racked in your server room, under your desk, or in a co-lo somewhere. It can be a cheap and convenient way to re-use existing hardware. Make sure you segment or isolate the agent into its own subnet, VLAN, DMZ, or VPC so that it cannot access the rest of your network. If you are thinking of running an actuated runner at home, we can share some iptables rules that worked well for our own testing. If you need to build for ARM64, we've shown that a Raspberry Pi 4 is faster than emulating ARM on a GitHub Hosted Runner Bare-metal on the cloud (higher cost, convenient, high performance) You can provision bare-metal hosts in the cloud using any number of providers like AWS, Alibaba Cloud, Cherry Servers, Equinix Metal, FastHosts, OVHcloud, Scaleway and Vultr, see a list here For Intel/AMD builds on AWS, you'll need to use AWS i3.metal . For ARM64 builds on AWS, the a1.metal is ideal. If you're not using AWS, or want larger machines, Equinix Metal offer bare-metal as a service for both Intel/AMD and ARM64 machines. This option is both convenient and offers the highest performance available, however bare-metal machines tends to be priced higher than you may be used to with VMs. Bear in mind that you may be able to run a single, larger bare-metal machine where you used to need half a dozen cloud VMs, since actuated can schedule builds much more efficiently than the built-in self-hosted runner from GitHub. Cloud Virtual Machines (VMs) that support nested virtualization (lowest cost, convenient, mid-level performance) Both DigitalOcean and Google Compute Platform (GCP) (new customers get 300 USD free credits from GCP) support nested virtualisation on their Virtual Machines (VMs). This option may not have the raw speed and throughput of a dedicated, bare-metal host, but keeps costs low and is convenient for getting started. The recommended Operating System for an Actuated Agent is: Ubuntu Server 22.04 or Ubuntu Server 20.04.","title":"Decide where to run your agent"},{"location":"add-agent/#review-the-end-user-license-agreement-eula","text":"Make sure you've read the Actuated EULA before registering your organisation with the actuated GitHub App, or starting the agent binary on one of your hosts.","title":"Review the End User License Agreement (EULA)"},{"location":"add-agent/#set-up-your-first-agent","text":"Download the agent and installation script Once you've decided where to set up your first agent, you'll need to download the installation package from a container registry Install crane : curl -sLS https://get.arkade.dev | sudo sh arkade get crane sudo mv crane /usr/local/bin/ rm -rf agent mkdir -p agent crane export ghcr.io/openfaasltd/actuated-agent:latest | tar -xvf - -C ./agent Install the agent binary to /usr/local/bin/ : sudo mv ./agent/agent* /usr/local/bin/ Run the setup.sh script which will install all the required dependencies like containerd, CNI and Firecracker. cd agent sudo ./install.sh Create a file to store your license from Gumroad: mkdir -p ~/.actuated # Paste the contents, hit enter, then Control + D # Or edit the file with nano/vim cat > $HOME /.actuated/LICENSE Generate an RSA keypair cd ~/.actuated/ agent keygen The RSA keypair is only used to encrypt messages and cannot. RSA keys are sometimes used with SSH sessions, however actuated does not use any form of SSH at this time. This will write: key_rsa and key_rsa.pub to the current working folder. Keep the key_rsa private, we will not ask you to share this file with us. Share key_rsa.pub with us via email or Slack. This key is not confidential, so don't worry about sharing it. cat ~/.actuated/key_rsa.pub Install the agent's authentication token. Create an API token for us to present when we send jobs to your Actuated Agent: openssl rand -base64 32 > ~/.actuated/TOKEN Encrypt the token with our public key and email .actuated/TOKEN.ENC to us, or share it with us on Slack: cat <<EOF > actuated.pem -----BEGIN RSA PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAo9EC7IrP8zTE9jm8agPa m0D/sFfmAlchhskLZksO4ZYzDHK9fuQ9oEhPYVkgrU5TifbL5UchdsSn//ELSy2Q TPRQoXVMdzPgLCrn15U+Xr7KpV3iNBV1go+ZzNE/ymdyS2kCCjxYiBLVuymn20hA ZzqkHSyOeM6IrG+A462KfmN0vqIpubpMkoK/wSkSSDjN0SoMWc9gaAqEFEHkSt9+ t65fIdzG0sKSEMb613WG+K/A/WBrcdqGHWoMG2h2CpK12tNobZEt3yCL0WVgkAKU VwaHniNYHn5niJHH/DgvXMWDECoKA1ZJyMdWC3MuIlyfWVzT5N7a/HPTzyzlrdCl bwIDAQAB -----END RSA PUBLIC KEY----- EOF agent encrypt --key ./actuated.pem \\ --in $HOME /.actuated/TOKEN \\ --out $HOME /.actuated/TOKEN.enc Post-pilot, we will provide a more automated way to exchange this token. Add HTTPS for the agent's endpoint The actuated control plane will only communicate with a HTTPS endpoint to ensure properly encryption is in place. An API token is used in addition with the TLS connection for all requests. In addition, any bootstrap tokens sent to the agent are further encrypted with the agent's public key. For hosts with public IPs, you will need to use the built-in TLS provisioning with Let's Encrypt. For hosts behind a firewall, NAT or in a private datacenter, you can use inlets to create a secure tunnel to the agent. We're considering other models for after the pilot, for instance GitHub's own API has the runner make an outbound connection and uses long-polling. See also: expose the agent with HTTPS Start the agent For an Intel/AMD Actuated Agent, create a start.sh file: #!/bin/bash echo Running Agent from: ./agent DOMAIN = agent1.example.com sudo -E agent up \\ --image-ref = ghcr.io/openfaasltd/actuated-ubuntu20.04:x86-64-dfb3ac12ae8d41ba00d5264e988256ce89acc9c6 \\ --kernel-ref = ghcr.io/openfaasltd/actuated-kernel-5.10.77:x86-64-dfb3ac12ae8d41ba00d5264e988256ce89acc9c6 \\ --letsencrypt-domain $DOMAIN \\ --letsencrypt-email webmaster@ $DOMAIN For an Actuated Agent behind an inlets tunnel : #!/bin/bash echo Running Agent from: ./agent sudo -E agent up \\ --image-ref = ghcr.io/openfaasltd/actuated-ubuntu20.04:aarch64-dfb3ac12ae8d41ba00d5264e988256ce89acc9c6 \\ --kernel-ref = ghcr.io/openfaasltd/actuated-kernel-5.10.77:aarch64-dfb3ac12ae8d41ba00d5264e988256ce89acc9c6 \\ --listen-addr 127 .0.0.1: For ARM64 Actuated Agents, change the prefix of the image tags from x86-64- to aarch64- You can also run the Actuated Agent software as a systemd unit file for automatic restarts and to start upon boot-up.","title":"Set up your first agent"},{"location":"add-agent/#next-steps","text":"You can now start your first build and see it run on your actuated agent. Start a build on your agent See also: Troubleshooting your agent","title":"Next steps"},{"location":"expose-agent/","text":"Expose the agent's API over HTTPS \u00b6 The actuated agent serves HTTP, and must be accessible by the actuated control plane. We expect most of our pilot customers to be using hosts with public IP addresses, and the combination of an API token plus TLS is a battle tested combination. For anyone running with private hosts, OpenFaaS Ltd's inlets product can be used to get incoming traffic over a secure tunnel For a host on a public cloud \u00b6 If you're running the agent on a host with a public IP, you can use the built-in TLS mechanism in the actuated agent to receive a certificate from Let's Encrypt, valid for 90 days. The certificate will be renewed by the actuated agent, so there are no additional administration tasks required. Pictured: Accessing the agent's endpoint built-in TLS and Let's Encrypt Determine the public IP of your instance: # curl -s http://checkip.amazonaws.com 141 .73.80.100 Now imagine that your sub-domain is agent.example.com , you need to create a DNS A record of agent.example.com=141.73.80.100 , changing both the sub-domain and IP to your own. Once created, edit the start.sh file on the agent and add two flags: --letsencrypt-domain agent.example.com \\ --letsencrypt-email webmaster@agent.example.com Your agent's endpoint URL is going to be: https://agent.example.com on port 443 Private hosts - on-premises, behind NAT or at home \u00b6 You'll need a way to expose the client to the Internet, which includes HTTPS encryption and a sufficient amount of connections/traffic per minute. Inlets provides a quick and secure solution here. It is available on a monthly subscription , bear in mind that the \"Personal\" plan is not for this kind of commercial use. Pictured: Accessing the agent's private endpoint using an inlets-pro tunnel Reach out to us if you'd like us to host a tunnel server for you, alternatively, you can follow the instructions below to set up your own. The inletsctl tool will create a HTTPS tunnel server with you on your favourite cloud with a HTTPS certificate obtained from Let's Encrypt. If you have just the one Actuated Agent: export AGENT_DOMAIN = agent1.example.com export LE_EMAIL = webmaster@agent1.example.com arkade get inletsctl sudo mv $HOME /.arkade/bin/inletsctl /usr/local/bin/ inletsctl create \\ --provider digitalocean \\ --region lon1 \\ --token-file $HOME /do-token \\ --letsencrypt-email $LE_EMAIL \\ --letsencrypt-domain $AGENT_DOMAIN Then note down the tunnel's wss:// URL and token. Then run a HTTPS client to expose your agent: inlets-pro http client \\ --url $WSS_URL \\ --token $TOKEN \\ --upstream http://127.0.0.1:8081 For two or more Actuated Agents: export AGENT_DOMAIN1 = agent1.example.com export AGENT_DOMAIN2 = agent2.example.com export LE_EMAIL = webmaster@agent1.example.com arkade get inletsctl sudo mv $HOME /.arkade/bin/inletsctl /usr/local/bin/ inletsctl create \\ --provider digitalocean \\ --region lon1 \\ --token-file $HOME /do-token \\ --letsencrypt-email $LE_EMAIL \\ --letsencrypt-domain $AGENT_DOMAIN1 \\ --letsencrypt-domain $AGENT_DOMAIN2 Then note down the tunnel's wss:// URL and token. Then run a HTTPS client to expose your agent, using the unique agent domain, run the inlets-pro client on the Actuated Agents: export AGENT_DOMAIN1 = agent1.example.com inlets-pro http client \\ --url $WSS_URL \\ --token $TOKEN \\ --upstream $AGENT1_DOMAIN = http://127.0.0.1:8081 export AGENT_DOMAIN2 = agent2.example.com inlets-pro http client \\ --url $WSS_URL \\ --token $TOKEN \\ --upstream $AGENT1_DOMAIN = http://127.0.0.1:8081 You can generate a systemd service (so that inlets restarts upon disconnection, and reboot) by adding --generate=systemd > inlets.service and running: sudo cp inlets.service /etc/systemd/system/ sudo systemctl daemon-reload sudo systemctl enable inlets.service sudo systemctl start inlets # Check status with: sudo systemctl status inlets Your agent's endpoint URL is going to be: https://$AGENT_DOMAIN . Preventing the runner from accessing your local network \u00b6 Network segmentation Proper network segmentation of hosts running the actuated agent is required. This is to prevent runners from making outbound connections to other hosts on your local network. We will not accept any responsibility for your configuration. If hardware isolation is not available, iptables rules may provide an alternative for isolating the runners from your network. Imagine you were using a LAN range of 192.168.0.0/24 , with a router of 192.168.0.1 , then the following probes and tests show that the runner cannot access the host 192.168.0.101, and that nmap's scan will come up dry. We add a rule to allow access to the router, but reject packets going via TCP or UDP to any other hosts on the network. sudo iptables --insert CNI-ADMIN \\ --destination 192 .168.0.1 --jump ACCEPT sudo iptables --insert CNI-ADMIN \\ --destination 192 .168.0.0/24 --jump REJECT -p tcp --reject-with tcp-reset sudo iptables --insert CNI-ADMIN \\ --destination 192 .168.0.0/24 --jump REJECT -p udp --reject-with icmp-port-unreachable You can test the efficacy of these rules by running nmap, mtr, ping and any other probing utilities within a GitHub workflow. name : CI on : pull_request : branches : - '*' push : branches : - master - main jobs : specs : name : specs runs-on : actuated steps : - uses : actions/checkout@v1 - name : addr run : ip addr - name : route run : ip route - name : pkgs run : | sudo apt-get update && \\ sudo apt-get install traceroute mtr nmap netcat -qy - name : traceroute run : traceroute 192.168.0.101 - name : Connect to ssh run : echo | nc 192.168.0.101 22 - name : mtr run : mtr -rw -c 1 192.168.0.101 - name : nmap for SSH run : nmap -p 22 192.168.0.0/24 - name : Ping router run : | ping -c 1 192.168.0.1 - name : Ping 101 run : | ping -c 1 192.168.0.101","title":"Expose agent"},{"location":"expose-agent/#expose-the-agents-api-over-https","text":"The actuated agent serves HTTP, and must be accessible by the actuated control plane. We expect most of our pilot customers to be using hosts with public IP addresses, and the combination of an API token plus TLS is a battle tested combination. For anyone running with private hosts, OpenFaaS Ltd's inlets product can be used to get incoming traffic over a secure tunnel","title":"Expose the agent's API over HTTPS"},{"location":"expose-agent/#for-a-host-on-a-public-cloud","text":"If you're running the agent on a host with a public IP, you can use the built-in TLS mechanism in the actuated agent to receive a certificate from Let's Encrypt, valid for 90 days. The certificate will be renewed by the actuated agent, so there are no additional administration tasks required. Pictured: Accessing the agent's endpoint built-in TLS and Let's Encrypt Determine the public IP of your instance: # curl -s http://checkip.amazonaws.com 141 .73.80.100 Now imagine that your sub-domain is agent.example.com , you need to create a DNS A record of agent.example.com=141.73.80.100 , changing both the sub-domain and IP to your own. Once created, edit the start.sh file on the agent and add two flags: --letsencrypt-domain agent.example.com \\ --letsencrypt-email webmaster@agent.example.com Your agent's endpoint URL is going to be: https://agent.example.com on port 443","title":"For a host on a public cloud"},{"location":"expose-agent/#private-hosts-on-premises-behind-nat-or-at-home","text":"You'll need a way to expose the client to the Internet, which includes HTTPS encryption and a sufficient amount of connections/traffic per minute. Inlets provides a quick and secure solution here. It is available on a monthly subscription , bear in mind that the \"Personal\" plan is not for this kind of commercial use. Pictured: Accessing the agent's private endpoint using an inlets-pro tunnel Reach out to us if you'd like us to host a tunnel server for you, alternatively, you can follow the instructions below to set up your own. The inletsctl tool will create a HTTPS tunnel server with you on your favourite cloud with a HTTPS certificate obtained from Let's Encrypt. If you have just the one Actuated Agent: export AGENT_DOMAIN = agent1.example.com export LE_EMAIL = webmaster@agent1.example.com arkade get inletsctl sudo mv $HOME /.arkade/bin/inletsctl /usr/local/bin/ inletsctl create \\ --provider digitalocean \\ --region lon1 \\ --token-file $HOME /do-token \\ --letsencrypt-email $LE_EMAIL \\ --letsencrypt-domain $AGENT_DOMAIN Then note down the tunnel's wss:// URL and token. Then run a HTTPS client to expose your agent: inlets-pro http client \\ --url $WSS_URL \\ --token $TOKEN \\ --upstream http://127.0.0.1:8081 For two or more Actuated Agents: export AGENT_DOMAIN1 = agent1.example.com export AGENT_DOMAIN2 = agent2.example.com export LE_EMAIL = webmaster@agent1.example.com arkade get inletsctl sudo mv $HOME /.arkade/bin/inletsctl /usr/local/bin/ inletsctl create \\ --provider digitalocean \\ --region lon1 \\ --token-file $HOME /do-token \\ --letsencrypt-email $LE_EMAIL \\ --letsencrypt-domain $AGENT_DOMAIN1 \\ --letsencrypt-domain $AGENT_DOMAIN2 Then note down the tunnel's wss:// URL and token. Then run a HTTPS client to expose your agent, using the unique agent domain, run the inlets-pro client on the Actuated Agents: export AGENT_DOMAIN1 = agent1.example.com inlets-pro http client \\ --url $WSS_URL \\ --token $TOKEN \\ --upstream $AGENT1_DOMAIN = http://127.0.0.1:8081 export AGENT_DOMAIN2 = agent2.example.com inlets-pro http client \\ --url $WSS_URL \\ --token $TOKEN \\ --upstream $AGENT1_DOMAIN = http://127.0.0.1:8081 You can generate a systemd service (so that inlets restarts upon disconnection, and reboot) by adding --generate=systemd > inlets.service and running: sudo cp inlets.service /etc/systemd/system/ sudo systemctl daemon-reload sudo systemctl enable inlets.service sudo systemctl start inlets # Check status with: sudo systemctl status inlets Your agent's endpoint URL is going to be: https://$AGENT_DOMAIN .","title":"Private hosts - on-premises, behind NAT or at home"},{"location":"expose-agent/#preventing-the-runner-from-accessing-your-local-network","text":"Network segmentation Proper network segmentation of hosts running the actuated agent is required. This is to prevent runners from making outbound connections to other hosts on your local network. We will not accept any responsibility for your configuration. If hardware isolation is not available, iptables rules may provide an alternative for isolating the runners from your network. Imagine you were using a LAN range of 192.168.0.0/24 , with a router of 192.168.0.1 , then the following probes and tests show that the runner cannot access the host 192.168.0.101, and that nmap's scan will come up dry. We add a rule to allow access to the router, but reject packets going via TCP or UDP to any other hosts on the network. sudo iptables --insert CNI-ADMIN \\ --destination 192 .168.0.1 --jump ACCEPT sudo iptables --insert CNI-ADMIN \\ --destination 192 .168.0.0/24 --jump REJECT -p tcp --reject-with tcp-reset sudo iptables --insert CNI-ADMIN \\ --destination 192 .168.0.0/24 --jump REJECT -p udp --reject-with icmp-port-unreachable You can test the efficacy of these rules by running nmap, mtr, ping and any other probing utilities within a GitHub workflow. name : CI on : pull_request : branches : - '*' push : branches : - master - main jobs : specs : name : specs runs-on : actuated steps : - uses : actions/checkout@v1 - name : addr run : ip addr - name : route run : ip route - name : pkgs run : | sudo apt-get update && \\ sudo apt-get install traceroute mtr nmap netcat -qy - name : traceroute run : traceroute 192.168.0.101 - name : Connect to ssh run : echo | nc 192.168.0.101 22 - name : mtr run : mtr -rw -c 1 192.168.0.101 - name : nmap for SSH run : nmap -p 22 192.168.0.0/24 - name : Ping router run : | ping -c 1 192.168.0.1 - name : Ping 101 run : | ping -c 1 192.168.0.101","title":"Preventing the runner from accessing your local network"},{"location":"faq/","text":"Frequently Asked Questions (FAQ) \u00b6 How does it work? \u00b6 Actuated has three main parts: an agent which knows how to run VMs, you install this on your hosts a VM image and Kernel that we build which has everything required for Docker, KinD and K3s a multi-tenant control plane that we host, which tells your agents to start VMs and register a runner on your GitHub organisation The multi-tenant control plane is run and operated by OpenFaaS Ltd. The conceptual overview showing how a MicroVM is requested by the control plane. MicroVMs are only started when needed, and are registered with GitHub by the official GitHub Actions runner, using a short-lived registration token. The token is been encrypted with the public key of the agent. This ensures no other agent could use the token to bootstrap a token to the wrong organisation. Learn more: Self-hosted GitHub Actions API How does actuated compare to a self-hosted runner? \u00b6 A self-hosted runner is a machine on which you've installed and registered the a GitHub runner. Quite often these machines suffer from some, if not all of the following issues: They require several hours to get all the required packages correctly installed to mirror a hosted runner You never update them out of fear of wasting time or breaking something which is working, meaning your supply chain is at risk Builds clash, if you're building a container image, or running a KinD cluster, names will clash, dirty state will be left over We've heard in user interviews that the final point of dirty state can cause engineers to waste several days of effort chasing down problems. Actuated uses a one-shot VM that is destroyed immediately after a build is completed. Who is actuated for? \u00b6 actuated is primarily for software engineering teams who are currently using GitHub Actions. A GitHub organisation is required for installation, and runners are attached to individual repositories as required, to execute builds. What kind of machines do I need for the agent? \u00b6 You'll need either: a bare-metal host (your own, AWS i3.metal or Equinix Metal), or a VM that supports nested virtualisation such as those provided by GCP and DigitalOcean. When will Jenkins, GitLab CI, BitBucket Pipeline Runners, Drone or Azure DevOps be supported? \u00b6 For the pilot phase, we're targeting GitHub Actions because it has fine-grained access controls and the ability to schedule exactly one build to a runner. Most other CI systems expect self-hosted runners to perform many builds, and we believe that to be an anti-pattern. We'll offer advice to teams accepted into the pilot who wish to evaluate GitHub Actions or migrate away from another solution. That said, if you're using these tools within your organisation, and face similar issues or concerns, we'd like to hear from you. Feel free to contact us at: contact@openfaas.com What kind of access is required to my GitHub Organisation? \u00b6 GitHub Apps provide fine-grained privileges, access control, and event data. Actuated integrates with GitHub using a GitHub App . The actuated GitHub App will request: Administrative access to add/remove GitHub Actions Runners to individual repositories Events via webhook for Workflow Runs and Workflow Jobs Can GitHub's self-hosted runner be used on public repos? \u00b6 The GitHub team recommends only running their self-hosted runners on private repositories. Why? On first glance, it seems like this might be due to how most people re-use a runner, and register it to process many jobs. It may even be because a bad actor could scan the local network of the runner and attempt to gain access to other systems. Actuated and iptables can largely fix both of these issues. So, can you use a self-hosted runner on a public repo? Through VM-level isolation, the primary concerns is resolved, because every run is started in an immutable VM. A bad actor could compromise the system or install malware leaving side-effects for future builds. The second issue is that a bad actor could use the runner to run network scans or attacks against remote hosts. This is a very hard problem to solve because a GitHub Action is a remote code execution (RCE) environment. With Actuated, we can restrict builds on public repositories to organisation members only. If that's of interest, let us know. How many builds does a single actuated VM run? \u00b6 When a VM starts up, it runs the GitHub Actions Runner ephemeral (aka one-shot) mode, so in can run at most one build. After that, the VM will be destroyed. See also: GitHub: ephemeral runners How are VMs scheduled? \u00b6 VMs are placed efficiently across your Actuated Agents using a simple algorithm based upon the amount of RAM reserved for the VM. Autoscaling of VMs is automatic. Let's say that you had 10 jobs pending, but given the RAM configuration, only enough capacity to run 8 of them? The second two would be queued until capacity one or more of those 8 jobs completed. If you find yourself regularly getting into a queued state, there are three potential changes to consider: Using Actuated Agents with more RAM Allocated less RAM to each job Adding more Actuated Agents The plan you select will determine how many Actuated Agents you can run, so consider 1. and 2. before 3. Do I need to auto-scale the Actuated Agents? \u00b6 If you haven't, read the previous section. Most teams that we've interviewed said that a small static pool of Actuated Agents would satisfy their build requirements. For the pilot period, we are not offering auto-scaling of Actuated Agents. If you feel that is a requirement for your team, set up some time to tell us why and we'll see if we can help. What's in the VM image and how is it built? \u00b6 The VM image contains similar software to the hosted runner image: ubuntu-latest offered by GitHub. Unfortunately, GitHub does not publish this image, so we've done our best through user-testing to reconstruct it, including all the Kernel modules required to run Kubernetes and Docker. The image is built automatically using GitHub Actions and is available on a container registry. How easy is it to debug a runner? \u00b6 OpenSSH is pre-installed, but it will be inaccessible from your workstation by default. So to connect to it, you can use an inlets tunnel , Wireguard VPN or Tailscale ephemeral token (beware, Tailscale is not free for your commercial use) to log into any agent. We recommend you add your SSH key and disable login with a password. We're also considering an automated SSH gateway and a convenient CLI for actuated customers. Let us know if you'd like to try this out. What do I need to change in my workflows? \u00b6 Very little, just add / set runs-on: actuated Is ARM64 supported? \u00b6 Yes, actuated is built to run on both Intel/AMD and ARM64 hosts, check your subscription plan to see if ARM64 is included. This includes a Raspberry Pi 4B, AWS Graviton, Oracle Cloud ARM instances and potentially any other ARM64 instances which support virtualisation. How does actuated compare to a actions-runtime-controller (ARC)? \u00b6 actions-runtime-controller (ARC)) is maintained by Yusuke Kuoka . Its primary use-case is scale GitHub's self-hosted actions runner using Pods in a Kubernetes cluster. ARC is self-hosted software which means its setup and operation are complex, requiring you to create an properly configure a GitHub App along with its keys. For actuated, you only need to run a single binary on each of your runner hosts and send us an encrypted bootstrap token. If you're running npm install or maven , then this may be a suitable isolation boundary for you. The default mode for ARC is a reuseable runner, which can run many jobs, and each job could leave side-effects or poison the runner for future job runs. If you need to build a container, in a container, on a Kubernetes node offers little isolation or security boundary. What if ARC is configured to use \"rootless\" containers? With a rootless container, you lose access to \"root\" and sudo , both of which are essential in any kind of CI job. Actuated users get full access to root, and can run docker build without any tricks or losing access to sudo . That's the same experience you get from a hosted runner by GitHub, but it's faster because it's on your own hardware. You can even run minikube, KinD, K3s and OpenShift with actuated without any changes. ARC runs a container, so that should work on any machine with a modern Kernel, however actuated runs a VM, in order to provide proper isolation. That means ARC runners can run pretty much anywhere, but actuated runners need to be on a bare-metal machine, or a VM that supports nested virtualisation. See also: Where can I run my agents? Are Windows or MacOS supported? \u00b6 Linux is the only supported platform for actuated at this time on a AMD64 or ARM64 architecture. We may consider other operating systems in the future, feel free to reach out to us. Doesn't Kaniko fix all this for ARC? \u00b6 Kaniko , by Google is an open source project for building containers. It's usually run as a container itself, and usually will require root privileges in order to mount the various filesystems layers required. If you're an ARC user and for various reasons, cannot migrate away to a more secure solution, Kaniko may be a step in the right direction. Google Cloud users could also create a dedicated node pool with gVisor enabled, for some additional isolation. However, it can only build containers, and still requires root, and itself is often run in Docker, so we're getting back to the same problems that actuated set out to solve. In addition, Kaniko cannot and will not help you to run that container that you've just built to validate it to run end to end tests, neither can it run a KinD cluster, or a Minikube cluster. Is Actuated free and open-source? \u00b6 Actuated is commercial software developed by OpenFaaS Ltd. A subscription will be required to use the software. Read the End User License Agreement (EULA) The website and documentation are available on GitHub and we plan to release some open source tools in the future for actuated customers. Is there a risk that we could get \"locked-in\" to actuated? \u00b6 No, you can move back to either hosted runners (pay per minute from GitHub) or self-managed self-hosted runners at any time. Bear in mind that actuated solves for a certain set of issues with both of those approaches. Why is the brand called \"actuated\" and \"selfactuated\"? \u00b6 The name of the software is actuated, in some places \"actuated\" is not available, and we liked \"selfactuated\" more than \"actuatedhq\" or \"actuatedio\" because it refers to the hybrid experience of self-hosted runners.","title":"FAQ"},{"location":"faq/#frequently-asked-questions-faq","text":"","title":"Frequently Asked Questions (FAQ)"},{"location":"faq/#how-does-it-work","text":"Actuated has three main parts: an agent which knows how to run VMs, you install this on your hosts a VM image and Kernel that we build which has everything required for Docker, KinD and K3s a multi-tenant control plane that we host, which tells your agents to start VMs and register a runner on your GitHub organisation The multi-tenant control plane is run and operated by OpenFaaS Ltd. The conceptual overview showing how a MicroVM is requested by the control plane. MicroVMs are only started when needed, and are registered with GitHub by the official GitHub Actions runner, using a short-lived registration token. The token is been encrypted with the public key of the agent. This ensures no other agent could use the token to bootstrap a token to the wrong organisation. Learn more: Self-hosted GitHub Actions API","title":"How does it work?"},{"location":"faq/#how-does-actuated-compare-to-a-self-hosted-runner","text":"A self-hosted runner is a machine on which you've installed and registered the a GitHub runner. Quite often these machines suffer from some, if not all of the following issues: They require several hours to get all the required packages correctly installed to mirror a hosted runner You never update them out of fear of wasting time or breaking something which is working, meaning your supply chain is at risk Builds clash, if you're building a container image, or running a KinD cluster, names will clash, dirty state will be left over We've heard in user interviews that the final point of dirty state can cause engineers to waste several days of effort chasing down problems. Actuated uses a one-shot VM that is destroyed immediately after a build is completed.","title":"How does actuated compare to a self-hosted runner?"},{"location":"faq/#who-is-actuated-for","text":"actuated is primarily for software engineering teams who are currently using GitHub Actions. A GitHub organisation is required for installation, and runners are attached to individual repositories as required, to execute builds.","title":"Who is actuated for?"},{"location":"faq/#what-kind-of-machines-do-i-need-for-the-agent","text":"You'll need either: a bare-metal host (your own, AWS i3.metal or Equinix Metal), or a VM that supports nested virtualisation such as those provided by GCP and DigitalOcean.","title":"What kind of machines do I need for the agent?"},{"location":"faq/#when-will-jenkins-gitlab-ci-bitbucket-pipeline-runners-drone-or-azure-devops-be-supported","text":"For the pilot phase, we're targeting GitHub Actions because it has fine-grained access controls and the ability to schedule exactly one build to a runner. Most other CI systems expect self-hosted runners to perform many builds, and we believe that to be an anti-pattern. We'll offer advice to teams accepted into the pilot who wish to evaluate GitHub Actions or migrate away from another solution. That said, if you're using these tools within your organisation, and face similar issues or concerns, we'd like to hear from you. Feel free to contact us at: contact@openfaas.com","title":"When will Jenkins, GitLab CI, BitBucket Pipeline Runners, Drone or Azure DevOps be supported?"},{"location":"faq/#what-kind-of-access-is-required-to-my-github-organisation","text":"GitHub Apps provide fine-grained privileges, access control, and event data. Actuated integrates with GitHub using a GitHub App . The actuated GitHub App will request: Administrative access to add/remove GitHub Actions Runners to individual repositories Events via webhook for Workflow Runs and Workflow Jobs","title":"What kind of access is required to my GitHub Organisation?"},{"location":"faq/#can-githubs-self-hosted-runner-be-used-on-public-repos","text":"The GitHub team recommends only running their self-hosted runners on private repositories. Why? On first glance, it seems like this might be due to how most people re-use a runner, and register it to process many jobs. It may even be because a bad actor could scan the local network of the runner and attempt to gain access to other systems. Actuated and iptables can largely fix both of these issues. So, can you use a self-hosted runner on a public repo? Through VM-level isolation, the primary concerns is resolved, because every run is started in an immutable VM. A bad actor could compromise the system or install malware leaving side-effects for future builds. The second issue is that a bad actor could use the runner to run network scans or attacks against remote hosts. This is a very hard problem to solve because a GitHub Action is a remote code execution (RCE) environment. With Actuated, we can restrict builds on public repositories to organisation members only. If that's of interest, let us know.","title":"Can GitHub's self-hosted runner be used on public repos?"},{"location":"faq/#how-many-builds-does-a-single-actuated-vm-run","text":"When a VM starts up, it runs the GitHub Actions Runner ephemeral (aka one-shot) mode, so in can run at most one build. After that, the VM will be destroyed. See also: GitHub: ephemeral runners","title":"How many builds does a single actuated VM run?"},{"location":"faq/#how-are-vms-scheduled","text":"VMs are placed efficiently across your Actuated Agents using a simple algorithm based upon the amount of RAM reserved for the VM. Autoscaling of VMs is automatic. Let's say that you had 10 jobs pending, but given the RAM configuration, only enough capacity to run 8 of them? The second two would be queued until capacity one or more of those 8 jobs completed. If you find yourself regularly getting into a queued state, there are three potential changes to consider: Using Actuated Agents with more RAM Allocated less RAM to each job Adding more Actuated Agents The plan you select will determine how many Actuated Agents you can run, so consider 1. and 2. before 3.","title":"How are VMs scheduled?"},{"location":"faq/#do-i-need-to-auto-scale-the-actuated-agents","text":"If you haven't, read the previous section. Most teams that we've interviewed said that a small static pool of Actuated Agents would satisfy their build requirements. For the pilot period, we are not offering auto-scaling of Actuated Agents. If you feel that is a requirement for your team, set up some time to tell us why and we'll see if we can help.","title":"Do I need to auto-scale the Actuated Agents?"},{"location":"faq/#whats-in-the-vm-image-and-how-is-it-built","text":"The VM image contains similar software to the hosted runner image: ubuntu-latest offered by GitHub. Unfortunately, GitHub does not publish this image, so we've done our best through user-testing to reconstruct it, including all the Kernel modules required to run Kubernetes and Docker. The image is built automatically using GitHub Actions and is available on a container registry.","title":"What's in the VM image and how is it built?"},{"location":"faq/#how-easy-is-it-to-debug-a-runner","text":"OpenSSH is pre-installed, but it will be inaccessible from your workstation by default. So to connect to it, you can use an inlets tunnel , Wireguard VPN or Tailscale ephemeral token (beware, Tailscale is not free for your commercial use) to log into any agent. We recommend you add your SSH key and disable login with a password. We're also considering an automated SSH gateway and a convenient CLI for actuated customers. Let us know if you'd like to try this out.","title":"How easy is it to debug a runner?"},{"location":"faq/#what-do-i-need-to-change-in-my-workflows","text":"Very little, just add / set runs-on: actuated","title":"What do I need to change in my workflows?"},{"location":"faq/#is-arm64-supported","text":"Yes, actuated is built to run on both Intel/AMD and ARM64 hosts, check your subscription plan to see if ARM64 is included. This includes a Raspberry Pi 4B, AWS Graviton, Oracle Cloud ARM instances and potentially any other ARM64 instances which support virtualisation.","title":"Is ARM64 supported?"},{"location":"faq/#how-does-actuated-compare-to-a-actions-runtime-controller-arc","text":"actions-runtime-controller (ARC)) is maintained by Yusuke Kuoka . Its primary use-case is scale GitHub's self-hosted actions runner using Pods in a Kubernetes cluster. ARC is self-hosted software which means its setup and operation are complex, requiring you to create an properly configure a GitHub App along with its keys. For actuated, you only need to run a single binary on each of your runner hosts and send us an encrypted bootstrap token. If you're running npm install or maven , then this may be a suitable isolation boundary for you. The default mode for ARC is a reuseable runner, which can run many jobs, and each job could leave side-effects or poison the runner for future job runs. If you need to build a container, in a container, on a Kubernetes node offers little isolation or security boundary. What if ARC is configured to use \"rootless\" containers? With a rootless container, you lose access to \"root\" and sudo , both of which are essential in any kind of CI job. Actuated users get full access to root, and can run docker build without any tricks or losing access to sudo . That's the same experience you get from a hosted runner by GitHub, but it's faster because it's on your own hardware. You can even run minikube, KinD, K3s and OpenShift with actuated without any changes. ARC runs a container, so that should work on any machine with a modern Kernel, however actuated runs a VM, in order to provide proper isolation. That means ARC runners can run pretty much anywhere, but actuated runners need to be on a bare-metal machine, or a VM that supports nested virtualisation. See also: Where can I run my agents?","title":"How does actuated compare to a actions-runtime-controller (ARC)?"},{"location":"faq/#are-windows-or-macos-supported","text":"Linux is the only supported platform for actuated at this time on a AMD64 or ARM64 architecture. We may consider other operating systems in the future, feel free to reach out to us.","title":"Are Windows or MacOS supported?"},{"location":"faq/#doesnt-kaniko-fix-all-this-for-arc","text":"Kaniko , by Google is an open source project for building containers. It's usually run as a container itself, and usually will require root privileges in order to mount the various filesystems layers required. If you're an ARC user and for various reasons, cannot migrate away to a more secure solution, Kaniko may be a step in the right direction. Google Cloud users could also create a dedicated node pool with gVisor enabled, for some additional isolation. However, it can only build containers, and still requires root, and itself is often run in Docker, so we're getting back to the same problems that actuated set out to solve. In addition, Kaniko cannot and will not help you to run that container that you've just built to validate it to run end to end tests, neither can it run a KinD cluster, or a Minikube cluster.","title":"Doesn't Kaniko fix all this for ARC?"},{"location":"faq/#is-actuated-free-and-open-source","text":"Actuated is commercial software developed by OpenFaaS Ltd. A subscription will be required to use the software. Read the End User License Agreement (EULA) The website and documentation are available on GitHub and we plan to release some open source tools in the future for actuated customers.","title":"Is Actuated free and open-source?"},{"location":"faq/#is-there-a-risk-that-we-could-get-locked-in-to-actuated","text":"No, you can move back to either hosted runners (pay per minute from GitHub) or self-managed self-hosted runners at any time. Bear in mind that actuated solves for a certain set of issues with both of those approaches.","title":"Is there a risk that we could get \"locked-in\" to actuated?"},{"location":"faq/#why-is-the-brand-called-actuated-and-selfactuated","text":"The name of the software is actuated, in some places \"actuated\" is not available, and we liked \"selfactuated\" more than \"actuatedhq\" or \"actuatedio\" because it refers to the hybrid experience of self-hosted runners.","title":"Why is the brand called \"actuated\" and \"selfactuated\"?"},{"location":"register/","text":"Register your GitHub Organisation \u00b6 Would you like to try out Actuated for your team and GitHub Organisation? What you'll need \u00b6 A GitHub organisation (you can also create a test organisation to try actuated out) One or more public or private repositories hosted in the organisation Administrative access to add our GitHub App One or more bare-metal hosts or VMs that support nested virtualisation (if you need a recommendation, feel free to ask) Register for the pilot \u00b6 Fill out the Google Form below with as much detail as you can, and we'll be in touch with you. If we think that your team is a good fit for the pilot program, we'll send you further instructions. Register interest for the pilot Install the GitHub App \u00b6 Once accepted into the pilot program, an administrator from your GitHub organisation will need to install our GitHub App. GitHub Apps provide fine-grained access controls for third parties integrating with GitHub. Learn more in the FAQ . End User License Agreement (EULA) Make sure you've read the Actuated EULA before registering your organisation with the actuated GitHub App. Next steps \u00b6 Then you'll Add your first agent","title":"Register your GitHub Organisation"},{"location":"register/#register-your-github-organisation","text":"Would you like to try out Actuated for your team and GitHub Organisation?","title":"Register your GitHub Organisation"},{"location":"register/#what-youll-need","text":"A GitHub organisation (you can also create a test organisation to try actuated out) One or more public or private repositories hosted in the organisation Administrative access to add our GitHub App One or more bare-metal hosts or VMs that support nested virtualisation (if you need a recommendation, feel free to ask)","title":"What you'll need"},{"location":"register/#register-for-the-pilot","text":"Fill out the Google Form below with as much detail as you can, and we'll be in touch with you. If we think that your team is a good fit for the pilot program, we'll send you further instructions. Register interest for the pilot","title":"Register for the pilot"},{"location":"register/#install-the-github-app","text":"Once accepted into the pilot program, an administrator from your GitHub organisation will need to install our GitHub App. GitHub Apps provide fine-grained access controls for third parties integrating with GitHub. Learn more in the FAQ . End User License Agreement (EULA) Make sure you've read the Actuated EULA before registering your organisation with the actuated GitHub App.","title":"Install the GitHub App"},{"location":"register/#next-steps","text":"Then you'll Add your first agent","title":"Next steps"},{"location":"roadmap/","text":"Roadmap \u00b6 Actuated is in a pilot phase, running builds for participating customers. The core product is functioning and we are dogfooding it for OpenFaaS and inlets. Our goal with the pilot is to prove that there's market fit for a solution like this, and if so, we'll invest more time in automation, user experience, agent autoscaling, dashboards and other polish. For now, if you're interested in participating and giving feedback, we believe actuated already solves pain at this stage. Shipped Firecracker MicroVM support for runners Fat VM image to match tooling installed by GitHub Actions KinD support for runner's Kernel K3s support for runner's Kernel ARM64 support, including Raspberry Pi 4B Efficient scheduling of jobs across fleet of agents Samples for K3s/KinD/Matrix builds and OpenFaaS functions Coming soon: Subscription plans delivered by Gumroad CLI or Portal for reviewing connected agents Consider alternatives to having the control plane talk to agents over HTTPS, such as long polling and the agent connecting to the control plane. Automated agent installation and bootstrap Autoscaling for agents","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"Actuated is in a pilot phase, running builds for participating customers. The core product is functioning and we are dogfooding it for OpenFaaS and inlets. Our goal with the pilot is to prove that there's market fit for a solution like this, and if so, we'll invest more time in automation, user experience, agent autoscaling, dashboards and other polish. For now, if you're interested in participating and giving feedback, we believe actuated already solves pain at this stage. Shipped Firecracker MicroVM support for runners Fat VM image to match tooling installed by GitHub Actions KinD support for runner's Kernel K3s support for runner's Kernel ARM64 support, including Raspberry Pi 4B Efficient scheduling of jobs across fleet of agents Samples for K3s/KinD/Matrix builds and OpenFaaS functions Coming soon: Subscription plans delivered by Gumroad CLI or Portal for reviewing connected agents Consider alternatives to having the control plane talk to agents over HTTPS, such as long polling and the agent connecting to the control plane. Automated agent installation and bootstrap Autoscaling for agents","title":"Roadmap"},{"location":"test-build/","text":"Start a build on your agent \u00b6 Once you've registered your GitHub organisation , and set up your first runner , you can either add actuated to an existing GitHub workflow, or create a test repository to see it in action. We suggest creating a test build so that you can see how everything works before moving over to an existing repository. Don't use a public repository Due to limitations in the design of GitHub's runner, we recommend using a private repository. Learn more in the FAQ . Create a test build \u00b6 This build will show you the specs, OS and Kernel name reported by the MicroVM. Create a test repository and a GitHub Action Create ./.github/workflows/ci.yaml : name : CI on : pull_request : branches : - '*' push : branches : - master - main jobs : specs : name : specs runs-on : actuated steps : - uses : actions/checkout@v1 - name : sleep run : | sleep 2 - name : Check specs run : | ./specs.sh Note that the runs-on: field says actuated and not ubuntu-latest . This is how the actuated control plane knows to send this job to your agent. Then add specs.sh to the root of the repository: #!/bin/bash echo Information on main disk df -h / echo Memory info free -h echo Total CPUs: echo CPUs: $( nproc ) echo CPU Model cat /proc/cpuinfo | grep \"model name\" echo Kernel and OS info uname -a echo OS cat /etc/os-release echo Egress IP: curl -s -L -S https://checkip.amazonaws.com Hit commit, and watch the VM boot up. Do you have any questions or comments? Feel free to reach out to us over Slack in the public channel for support. Enable an existing repository \u00b6 To add actuated to an existing repository, simply edit the workflow YAML file and change runs-on: to runs-on: actuated . If you want to go back to a hosted runner, edit the field back to runs-on: ubuntu-latest or whatever you used prior to that.","title":"Start a build on your agent"},{"location":"test-build/#start-a-build-on-your-agent","text":"Once you've registered your GitHub organisation , and set up your first runner , you can either add actuated to an existing GitHub workflow, or create a test repository to see it in action. We suggest creating a test build so that you can see how everything works before moving over to an existing repository. Don't use a public repository Due to limitations in the design of GitHub's runner, we recommend using a private repository. Learn more in the FAQ .","title":"Start a build on your agent"},{"location":"test-build/#create-a-test-build","text":"This build will show you the specs, OS and Kernel name reported by the MicroVM. Create a test repository and a GitHub Action Create ./.github/workflows/ci.yaml : name : CI on : pull_request : branches : - '*' push : branches : - master - main jobs : specs : name : specs runs-on : actuated steps : - uses : actions/checkout@v1 - name : sleep run : | sleep 2 - name : Check specs run : | ./specs.sh Note that the runs-on: field says actuated and not ubuntu-latest . This is how the actuated control plane knows to send this job to your agent. Then add specs.sh to the root of the repository: #!/bin/bash echo Information on main disk df -h / echo Memory info free -h echo Total CPUs: echo CPUs: $( nproc ) echo CPU Model cat /proc/cpuinfo | grep \"model name\" echo Kernel and OS info uname -a echo OS cat /etc/os-release echo Egress IP: curl -s -L -S https://checkip.amazonaws.com Hit commit, and watch the VM boot up. Do you have any questions or comments? Feel free to reach out to us over Slack in the public channel for support.","title":"Create a test build"},{"location":"test-build/#enable-an-existing-repository","text":"To add actuated to an existing repository, simply edit the workflow YAML file and change runs-on: to runs-on: actuated . If you want to go back to a hosted runner, edit the field back to runs-on: ubuntu-latest or whatever you used prior to that.","title":"Enable an existing repository"},{"location":"troubleshooting/","text":"Troubleshooting \u00b6 Getting support \u00b6 All customers have access to a public Slack channel for support and collaboration. Enterprise customers may also have an upgraded SLA for support tickets via email and access to a private Slack channel. You need to rotate the authentication token used for your agent \u00b6 There should not be many reasons to rotate this token, however, if something's happened and it's been leaked or an employee has left the company, contact us via email for the update procedure. You need to rotate your private/public keypair \u00b6 Your private/public keypair is comparable to an SSH key, although it cannot be used to gain access to your agent via SSH. If you need to rotate it for some reason, please contact us by email as soon as you can. Disk space is running out \u00b6 This can be observed by running df -h or df -h / . Over time, the various \"fat\" VM images that we ship to you may fill up the disk space on your machine. You can delete all images, including the current image with the following command. sudo ctr -n mvm image ls -q | xargs sudo ctr -n mvm image rm We do not recommend running this on a cron schedule since the maintenance command will cause your agent to download the latest fat VM image ~ 1GB\u00b1 again. An alternative for a cron schedule would need to exclude the current image being used: CURRENT = \"ghcr.io/openfaasltd/actuated-ubuntu:20.0.4-2022-09-30-1357\" sudo ctr -n mvm image ls -q | grep -v $CURRENT | xargs sudo ctr -n mvm image rm Your agent has been offline or unavailable for a significant period of time \u00b6 If your agent has been offline for a significant period of time, then our control plane will have disconnected it from its pool of available agents. Contact us via Slack to have it reinstated. The devmapper snapshotter is not available or running \u00b6 The actuated agent uses the devmapper snapshotter for containerd, which emulates a thin-provisioned block device. Performance can be improved by attaching a dedicated disk or partition, but in our testing the devmapper works well enough for most workloads. The dmsetup.sh script must be run upon every fresh boot-up of the host. It enables Firecracker to use snapshots to save disk space when launching new VMs. If you see an error about the \"devmapper\" snapshot driver, then run the dmsetup.sh shell script then restart containerd: ./dmsetup.sh sudo systemctl daemon-reload sudo systemctl restart containerd Your builds are slower than expected \u00b6 Check free disk space ( df -h ) Check for unattended updates/upgrades ( ps -ef | grep unattended-upgrades ) and ( ps -ef | grep apt ) If you're using spinning disks, then consider switching to SSDs. If you're already using SSDs, consider using PCIe/NVMe SSDs. Finally, we do have another way to speed up microVMs by attaching another drive or partition to your host. Contact us for more information.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshooting/#getting-support","text":"All customers have access to a public Slack channel for support and collaboration. Enterprise customers may also have an upgraded SLA for support tickets via email and access to a private Slack channel.","title":"Getting support"},{"location":"troubleshooting/#you-need-to-rotate-the-authentication-token-used-for-your-agent","text":"There should not be many reasons to rotate this token, however, if something's happened and it's been leaked or an employee has left the company, contact us via email for the update procedure.","title":"You need to rotate the authentication token used for your agent"},{"location":"troubleshooting/#you-need-to-rotate-your-privatepublic-keypair","text":"Your private/public keypair is comparable to an SSH key, although it cannot be used to gain access to your agent via SSH. If you need to rotate it for some reason, please contact us by email as soon as you can.","title":"You need to rotate your private/public keypair"},{"location":"troubleshooting/#disk-space-is-running-out","text":"This can be observed by running df -h or df -h / . Over time, the various \"fat\" VM images that we ship to you may fill up the disk space on your machine. You can delete all images, including the current image with the following command. sudo ctr -n mvm image ls -q | xargs sudo ctr -n mvm image rm We do not recommend running this on a cron schedule since the maintenance command will cause your agent to download the latest fat VM image ~ 1GB\u00b1 again. An alternative for a cron schedule would need to exclude the current image being used: CURRENT = \"ghcr.io/openfaasltd/actuated-ubuntu:20.0.4-2022-09-30-1357\" sudo ctr -n mvm image ls -q | grep -v $CURRENT | xargs sudo ctr -n mvm image rm","title":"Disk space is running out"},{"location":"troubleshooting/#your-agent-has-been-offline-or-unavailable-for-a-significant-period-of-time","text":"If your agent has been offline for a significant period of time, then our control plane will have disconnected it from its pool of available agents. Contact us via Slack to have it reinstated.","title":"Your agent has been offline or unavailable for a significant period of time"},{"location":"troubleshooting/#the-devmapper-snapshotter-is-not-available-or-running","text":"The actuated agent uses the devmapper snapshotter for containerd, which emulates a thin-provisioned block device. Performance can be improved by attaching a dedicated disk or partition, but in our testing the devmapper works well enough for most workloads. The dmsetup.sh script must be run upon every fresh boot-up of the host. It enables Firecracker to use snapshots to save disk space when launching new VMs. If you see an error about the \"devmapper\" snapshot driver, then run the dmsetup.sh shell script then restart containerd: ./dmsetup.sh sudo systemctl daemon-reload sudo systemctl restart containerd","title":"The devmapper snapshotter is not available or running"},{"location":"troubleshooting/#your-builds-are-slower-than-expected","text":"Check free disk space ( df -h ) Check for unattended updates/upgrades ( ps -ef | grep unattended-upgrades ) and ( ps -ef | grep apt ) If you're using spinning disks, then consider switching to SSDs. If you're already using SSDs, consider using PCIe/NVMe SSDs. Finally, we do have another way to speed up microVMs by attaching another drive or partition to your host. Contact us for more information.","title":"Your builds are slower than expected"},{"location":"examples/debug-ssh/","text":"Example: Debug a job with SSH \u00b6 If your tier and subscription includes debugging with SSH, then you can follow these instructions to get a shell into your self-hosted runner. Certified for: x86_64 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ . Try out the action on your agent \u00b6 Create a secret for the repo or organisation for SSH_GATEWAY_IP using the IP address, or DNS address that you were provided with by your support team. Create a .github/workflows/workflow.yaml file name : connect on : pull_request : branches : - '*' push : branches : - master permissions : id-token : write contents : read actions : read jobs : connect : name : connect runs-on : actuated steps : - name : Setup SSH server for Actor uses : alexellis/setup-sshd-actor@master - name : Connect to the actuated SSH gateway uses : alexellis/actuated-ssh-gateway-action@master with : gatewayaddr : ${{ secrets.SSH_GATEWAY_IP }} secure : true - name : Setup a blocking tmux session uses : alexellis/block-with-tmux-action@master Next, trigger a build. Open https://$SSH_GATEWAY_IP/list in your browser and look for your session, you can log in using the SSH command outputted for you. Watch a demo:","title":"Debug a job with SSH"},{"location":"examples/debug-ssh/#example-debug-a-job-with-ssh","text":"If your tier and subscription includes debugging with SSH, then you can follow these instructions to get a shell into your self-hosted runner. Certified for: x86_64 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ .","title":"Example: Debug a job with SSH"},{"location":"examples/debug-ssh/#try-out-the-action-on-your-agent","text":"Create a secret for the repo or organisation for SSH_GATEWAY_IP using the IP address, or DNS address that you were provided with by your support team. Create a .github/workflows/workflow.yaml file name : connect on : pull_request : branches : - '*' push : branches : - master permissions : id-token : write contents : read actions : read jobs : connect : name : connect runs-on : actuated steps : - name : Setup SSH server for Actor uses : alexellis/setup-sshd-actor@master - name : Connect to the actuated SSH gateway uses : alexellis/actuated-ssh-gateway-action@master with : gatewayaddr : ${{ secrets.SSH_GATEWAY_IP }} secure : true - name : Setup a blocking tmux session uses : alexellis/block-with-tmux-action@master Next, trigger a build. Open https://$SSH_GATEWAY_IP/list in your browser and look for your session, you can log in using the SSH command outputted for you. Watch a demo:","title":"Try out the action on your agent"},{"location":"examples/docker/","text":"Example: Kubernetes with KinD \u00b6 Docker CE is preinstalled in the actuated VM image, and will start upon boot-up. Certified for: x86_64 arm64 including Raspberry Pi 4 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ . Try out the action on your agent \u00b6 Create a new file at: .github/workspaces/build.yml and commit it to the repository. Try running a container to ping Google for 3 times: name : build on : push jobs : ping-google : runs-on : actuated steps : - uses : actions/checkout@master with : fetch-depth : 1 - name : Run a ping to Google with Docker run : | docker run --rm -i alpine:latest ping -c 3 google.com Build a container with Docker: name : build on : push jobs : build-in-docker : runs-on : actuated steps : - uses : actions/checkout@master with : fetch-depth : 1 - name : Build inlets-connect using Docker run : | git clone --depth=1 https://github.com/alexellis/inlets-connect cd inlets-connect docker build -t inlets-connect . docker images To run this on ARM64, just change the actuated label to actuated-aarch64 .","title":"Docker run/build"},{"location":"examples/docker/#example-kubernetes-with-kind","text":"Docker CE is preinstalled in the actuated VM image, and will start upon boot-up. Certified for: x86_64 arm64 including Raspberry Pi 4 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ .","title":"Example: Kubernetes with KinD"},{"location":"examples/docker/#try-out-the-action-on-your-agent","text":"Create a new file at: .github/workspaces/build.yml and commit it to the repository. Try running a container to ping Google for 3 times: name : build on : push jobs : ping-google : runs-on : actuated steps : - uses : actions/checkout@master with : fetch-depth : 1 - name : Run a ping to Google with Docker run : | docker run --rm -i alpine:latest ping -c 3 google.com Build a container with Docker: name : build on : push jobs : build-in-docker : runs-on : actuated steps : - uses : actions/checkout@master with : fetch-depth : 1 - name : Build inlets-connect using Docker run : | git clone --depth=1 https://github.com/alexellis/inlets-connect cd inlets-connect docker build -t inlets-connect . docker images To run this on ARM64, just change the actuated label to actuated-aarch64 .","title":"Try out the action on your agent"},{"location":"examples/k3s/","text":"Example: Kubernetes with k3s \u00b6 You may need to access Kubernetes within your build. K3s is a for-production, lightweight distribution of Kubernetes that uses fewer resources than upstream. k3sup is a popular tool for installing k3s. Certified for: x86_64 arm64 including Raspberry Pi 4 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ . Try out the action on your agent \u00b6 Create a new file at: .github/workspaces/build.yml and commit it to the repository. Note that it's important to make sure Kubernetes is responsive before performing any commands like running a Pod or installing a helm chart. name : k3sup-tester on : push jobs : k3sup-tester : runs-on : actuated steps : - name : get arkade uses : alexellis/setup-arkade@v1 - name : get k3sup and kubectl uses : alexellis/arkade-get@master with : kubectl : latest k3sup : latest - name : Install K3s with k3sup run : | mkdir -p $HOME/.kube/ k3sup install --local --local-path $HOME/.kube/config - name : Wait until nodes ready run : | k3sup ready --quiet --kubeconfig $HOME/.kube/config --context default - name : Wait until CoreDNS is ready run : | kubectl rollout status deploy/coredns -n kube-system --timeout=300s - name : Explore nodes run : kubectl get nodes -o wide - name : Explore pods run : kubectl get pod -A -o wide To run this on ARM64, just change the actuated label to actuated-aarch64 .","title":"Kubernetes with K3s"},{"location":"examples/k3s/#example-kubernetes-with-k3s","text":"You may need to access Kubernetes within your build. K3s is a for-production, lightweight distribution of Kubernetes that uses fewer resources than upstream. k3sup is a popular tool for installing k3s. Certified for: x86_64 arm64 including Raspberry Pi 4 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ .","title":"Example: Kubernetes with k3s"},{"location":"examples/k3s/#try-out-the-action-on-your-agent","text":"Create a new file at: .github/workspaces/build.yml and commit it to the repository. Note that it's important to make sure Kubernetes is responsive before performing any commands like running a Pod or installing a helm chart. name : k3sup-tester on : push jobs : k3sup-tester : runs-on : actuated steps : - name : get arkade uses : alexellis/setup-arkade@v1 - name : get k3sup and kubectl uses : alexellis/arkade-get@master with : kubectl : latest k3sup : latest - name : Install K3s with k3sup run : | mkdir -p $HOME/.kube/ k3sup install --local --local-path $HOME/.kube/config - name : Wait until nodes ready run : | k3sup ready --quiet --kubeconfig $HOME/.kube/config --context default - name : Wait until CoreDNS is ready run : | kubectl rollout status deploy/coredns -n kube-system --timeout=300s - name : Explore nodes run : kubectl get nodes -o wide - name : Explore pods run : kubectl get pod -A -o wide To run this on ARM64, just change the actuated label to actuated-aarch64 .","title":"Try out the action on your agent"},{"location":"examples/kernel/","text":"Example: Test that compute time by compiling a Kernel \u00b6 Use this sample to test the raw compute speed of your hosts by building a Kernel. Certified for: x86_64 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ . Try out the action on your agent \u00b6 Create a new file at: .github/workspaces/build.yml and commit it to the repository. name : microvm-kernel on : push jobs : microvm-kernel : runs-on : actuated steps : - name : free RAM run : free -h - name : List CPUs run : nproc - name : get build toolchain run : | sudo apt update -qy sudo apt-get install -qy \\ git \\ build-essential \\ kernel-package \\ fakeroot \\ libncurses5-dev \\ libssl-dev \\ ccache \\ bison \\ flex \\ libelf-dev \\ dwarves - name : clone linux run : | time git clone https://github.com/torvalds/linux.git linux.git --depth=1 --branch v5.10 cd linux.git curl -o .config -s -f https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/resources/guest_configs/microvm-kernel-x86_64-5.10.config echo \"# CONFIG_KASAN is not set\" >> .config - name : make config run : | cd linux.git make oldconfig - name : Make vmlinux run : | cd linux.git time make vmlinux -j$(nproc) du -h ./vmlinux When you have a build time, why not change runs-on: actuated to runs-on: ubuntu-latest to compare it to a hosted runner from GitHub? Here's our test, where our own machine built the Kernel 4x faster than a hosted runner:","title":"Compile a Kernel"},{"location":"examples/kernel/#example-test-that-compute-time-by-compiling-a-kernel","text":"Use this sample to test the raw compute speed of your hosts by building a Kernel. Certified for: x86_64 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ .","title":"Example: Test that compute time by compiling a Kernel"},{"location":"examples/kernel/#try-out-the-action-on-your-agent","text":"Create a new file at: .github/workspaces/build.yml and commit it to the repository. name : microvm-kernel on : push jobs : microvm-kernel : runs-on : actuated steps : - name : free RAM run : free -h - name : List CPUs run : nproc - name : get build toolchain run : | sudo apt update -qy sudo apt-get install -qy \\ git \\ build-essential \\ kernel-package \\ fakeroot \\ libncurses5-dev \\ libssl-dev \\ ccache \\ bison \\ flex \\ libelf-dev \\ dwarves - name : clone linux run : | time git clone https://github.com/torvalds/linux.git linux.git --depth=1 --branch v5.10 cd linux.git curl -o .config -s -f https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/resources/guest_configs/microvm-kernel-x86_64-5.10.config echo \"# CONFIG_KASAN is not set\" >> .config - name : make config run : | cd linux.git make oldconfig - name : Make vmlinux run : | cd linux.git time make vmlinux -j$(nproc) du -h ./vmlinux When you have a build time, why not change runs-on: actuated to runs-on: ubuntu-latest to compare it to a hosted runner from GitHub? Here's our test, where our own machine built the Kernel 4x faster than a hosted runner:","title":"Try out the action on your agent"},{"location":"examples/kind/","text":"Example: Kubernetes with KinD \u00b6 You may need to access Kubernetes within your build. KinD is a popular option, and easy to run in an action. Certified for: x86_64 arm64 including Raspberry Pi 4 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ . Try out the action on your agent \u00b6 Create a new file at: .github/workspaces/build.yml and commit it to the repository. Note that it's important to make sure Kubernetes is responsive before performing any commands like running a Pod or installing a helm chart. name : build on : push jobs : start-kind : runs-on : actuated steps : - uses : actions/checkout@master with : fetch-depth : 1 - name : get arkade uses : alexellis/setup-arkade@v1 - name : get kubectl and kubectl uses : alexellis/arkade-get@master with : kubectl : latest kind : latest - name : Install Kubernetes kind run : | mkdir -p $HOME/.kube/ kind create cluster --wait 300s - name : Wait until CoreDNS is ready run : | kubectl rollout status deploy/coredns -n kube-system --timeout=300s - name : Explore nodes run : kubectl get nodes -o wide - name : Explore pods run : kubectl get pod -A -o wide - name : Show kubelet logs run : docker exec kind-control-plane journalctl -u kubelet To run this on ARM64, just change the actuated label to actuated-aarch64 .","title":"Kubernetes with KinD"},{"location":"examples/kind/#example-kubernetes-with-kind","text":"You may need to access Kubernetes within your build. KinD is a popular option, and easy to run in an action. Certified for: x86_64 arm64 including Raspberry Pi 4 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ .","title":"Example: Kubernetes with KinD"},{"location":"examples/kind/#try-out-the-action-on-your-agent","text":"Create a new file at: .github/workspaces/build.yml and commit it to the repository. Note that it's important to make sure Kubernetes is responsive before performing any commands like running a Pod or installing a helm chart. name : build on : push jobs : start-kind : runs-on : actuated steps : - uses : actions/checkout@master with : fetch-depth : 1 - name : get arkade uses : alexellis/setup-arkade@v1 - name : get kubectl and kubectl uses : alexellis/arkade-get@master with : kubectl : latest kind : latest - name : Install Kubernetes kind run : | mkdir -p $HOME/.kube/ kind create cluster --wait 300s - name : Wait until CoreDNS is ready run : | kubectl rollout status deploy/coredns -n kube-system --timeout=300s - name : Explore nodes run : kubectl get nodes -o wide - name : Explore pods run : kubectl get pod -A -o wide - name : Show kubelet logs run : docker exec kind-control-plane journalctl -u kubelet To run this on ARM64, just change the actuated label to actuated-aarch64 .","title":"Try out the action on your agent"},{"location":"examples/matrix/","text":"Example: matrix-build - run a VM per each job in a matrix \u00b6 Use this sample to test launching multiple VMs in parallel. Certified for: x86_64 arm64 including Raspberry Pi 4 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ . Try out the action on your agent \u00b6 Create a new file at: .github/workspaces/build.yml and commit it to the repository. name : CI on : pull_request : branches : - '*' push : branches : - master - main jobs : arkade-e2e : name : arkade-e2e runs-on : actuated strategy : matrix : apps : [ run-job , k3sup , arkade , kubectl , faas-cli ] steps : - name : Get arkade run : | curl -sLS https://get.arkade.dev | sudo sh - name : Download app run : | echo ${{ matrix.apps }} arkade get ${{ matrix.apps }} file /home/runner/.arkade/bin/${{ matrix.apps }} The matrix will cause a new VM to be launched for each item in the \"apps\" array.","title":"Job Matrix"},{"location":"examples/matrix/#example-matrix-build-run-a-vm-per-each-job-in-a-matrix","text":"Use this sample to test launching multiple VMs in parallel. Certified for: x86_64 arm64 including Raspberry Pi 4 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ .","title":"Example: matrix-build - run a VM per each job in a matrix"},{"location":"examples/matrix/#try-out-the-action-on-your-agent","text":"Create a new file at: .github/workspaces/build.yml and commit it to the repository. name : CI on : pull_request : branches : - '*' push : branches : - master - main jobs : arkade-e2e : name : arkade-e2e runs-on : actuated strategy : matrix : apps : [ run-job , k3sup , arkade , kubectl , faas-cli ] steps : - name : Get arkade run : | curl -sLS https://get.arkade.dev | sudo sh - name : Download app run : | echo ${{ matrix.apps }} arkade get ${{ matrix.apps }} file /home/runner/.arkade/bin/${{ matrix.apps }} The matrix will cause a new VM to be launched for each item in the \"apps\" array.","title":"Try out the action on your agent"},{"location":"examples/openfaas-helm/","text":"Example: Publish an OpenFaaS function \u00b6 This example will create a Kubernetes cluster using KinD, deploy OpenFaaS using Helm, deploy a function, then invoke the function. There are some additional checks for readiness for Kubernetes and the OpenFaaS gateway. You can adapt this example for any other Helm charts you may have for E2E testing. We also recommend considering arkade for installing CLIs and common Helm charts for testing. Docker CE is preinstalled in the actuated VM image, and will start upon boot-up. Certified for: x86_64 arm64 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ . Try out the action on your agent \u00b6 Create a new GitHub repository in your organisation. Add: .github/workflows/e2e.yaml name : e2e on : push : branches : - '*' pull_request : branches : - '*' permissions : actions : read contents : read jobs : e2e : runs-on : actuated steps : - uses : actions/checkout@master with : fetch-depth : 1 - name : get arkade uses : alexellis/setup-arkade@v1 - name : get kubectl and kubectl uses : alexellis/arkade-get@master with : kubectl : latest kind : latest faas-cli : latest - name : Install Kubernetes KinD run : | mkdir -p $HOME/.kube/ kind create cluster --wait 300s - name : Add Helm chart, update repos and apply namespaces run : | kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml helm repo add openfaas https://openfaas.github.io/faas-netes/ helm repo update - name : Install the Community Edition (CE) run : | helm repo update \\ && helm upgrade openfaas --install openfaas/openfaas \\ --namespace openfaas \\ --set functionNamespace=openfaas-fn \\ --set generateBasicAuth=true - name : Wait until OpenFaaS is ready run : | kubectl rollout status -n openfaas deploy/prometheus --timeout 5m kubectl rollout status -n openfaas deploy/gateway --timeout 5m - name : Port forward the gateway run : | kubectl port-forward -n openfaas svc/gateway 8080:8080 & attempts=0 max=10 until $(curl --output /dev/null --silent --fail http://127.0.0.1:8080/healthz ); do if [ ${attempts} -eq ${max} ]; then echo \"Max attempts reached $max waiting for gateway's health endpoint\" exit 1 fi printf '.' attempts=$(($attempts+1)) sleep 1 done - name : Login to OpenFaaS gateway and deploy a function run : | PASSWORD=$(kubectl get secret -n openfaas basic-auth -o jsonpath=\"{.data.basic-auth-password}\" | base64 --decode; echo) echo -n $PASSWORD | faas-cli login --username admin --password-stdin faas-cli store deploy env faas-cli invoke env <<< \"\" curl -s -f -i http://127.0.0.1:8080/function/env faas-cli invoke --async env <<< \"\" kubectl logs -n openfaas deploy/queue-worker faas-cli describe env If you'd like to deploy the function, check out a more comprehensive example of how to log in and deploy in Serverless For Everyone Else","title":"Deploy a Helm chart"},{"location":"examples/openfaas-helm/#example-publish-an-openfaas-function","text":"This example will create a Kubernetes cluster using KinD, deploy OpenFaaS using Helm, deploy a function, then invoke the function. There are some additional checks for readiness for Kubernetes and the OpenFaaS gateway. You can adapt this example for any other Helm charts you may have for E2E testing. We also recommend considering arkade for installing CLIs and common Helm charts for testing. Docker CE is preinstalled in the actuated VM image, and will start upon boot-up. Certified for: x86_64 arm64 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ .","title":"Example: Publish an OpenFaaS function"},{"location":"examples/openfaas-helm/#try-out-the-action-on-your-agent","text":"Create a new GitHub repository in your organisation. Add: .github/workflows/e2e.yaml name : e2e on : push : branches : - '*' pull_request : branches : - '*' permissions : actions : read contents : read jobs : e2e : runs-on : actuated steps : - uses : actions/checkout@master with : fetch-depth : 1 - name : get arkade uses : alexellis/setup-arkade@v1 - name : get kubectl and kubectl uses : alexellis/arkade-get@master with : kubectl : latest kind : latest faas-cli : latest - name : Install Kubernetes KinD run : | mkdir -p $HOME/.kube/ kind create cluster --wait 300s - name : Add Helm chart, update repos and apply namespaces run : | kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml helm repo add openfaas https://openfaas.github.io/faas-netes/ helm repo update - name : Install the Community Edition (CE) run : | helm repo update \\ && helm upgrade openfaas --install openfaas/openfaas \\ --namespace openfaas \\ --set functionNamespace=openfaas-fn \\ --set generateBasicAuth=true - name : Wait until OpenFaaS is ready run : | kubectl rollout status -n openfaas deploy/prometheus --timeout 5m kubectl rollout status -n openfaas deploy/gateway --timeout 5m - name : Port forward the gateway run : | kubectl port-forward -n openfaas svc/gateway 8080:8080 & attempts=0 max=10 until $(curl --output /dev/null --silent --fail http://127.0.0.1:8080/healthz ); do if [ ${attempts} -eq ${max} ]; then echo \"Max attempts reached $max waiting for gateway's health endpoint\" exit 1 fi printf '.' attempts=$(($attempts+1)) sleep 1 done - name : Login to OpenFaaS gateway and deploy a function run : | PASSWORD=$(kubectl get secret -n openfaas basic-auth -o jsonpath=\"{.data.basic-auth-password}\" | base64 --decode; echo) echo -n $PASSWORD | faas-cli login --username admin --password-stdin faas-cli store deploy env faas-cli invoke env <<< \"\" curl -s -f -i http://127.0.0.1:8080/function/env faas-cli invoke --async env <<< \"\" kubectl logs -n openfaas deploy/queue-worker faas-cli describe env If you'd like to deploy the function, check out a more comprehensive example of how to log in and deploy in Serverless For Everyone Else","title":"Try out the action on your agent"},{"location":"examples/openfaas-publish/","text":"Example: Publish an OpenFaaS function \u00b6 This example will publish an OpenFaaS function to GitHub's Container Registry (GHCR). The example uses Docker's buildx and QEMU for a multi-arch build Dynamic variables to inject the SHA and OWNER name from the repo Uses the token that GitHub assigns to the action to publish the containers. You can also run this example on GitHub's own hosted runners. Docker CE is preinstalled in the actuated VM image, and will start upon boot-up. Certified for: x86_64 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ . Try out the action on your agent \u00b6 For alexellis' repository called alexellis/autoscaling-functions , then check out the .github/workspaces/publish.yml file: The \"Setup QEMU\" and \"Set up Docker Buildx\" steps configure the builder to produce a multi-arch image. The \"OWNER\" variable means this action can be run on any organisation without having to hard-code a username for GHCR. Only the bcrypt function is being built with the --filter command added, remove it to build all functions in the stack.yml. --platforms linux/amd64,linux/arm64,linux/arm/v7 will build for regular Intel/AMD machines, 64-bit ARM and 32-bit ARM i.e. Raspberry Pi, most users can reduce this list to just \"linux/amd64\" for a speed improvement Make sure you edit runs-on: and set it to runs-on: actuated name : publish on : push : branches : - '*' pull_request : branches : - '*' permissions : actions : read checks : write contents : read packages : write jobs : publish : runs-on : actuated steps : - uses : actions/checkout@master with : fetch-depth : 1 - name : Get faas-cli run : curl -sLSf https://cli.openfaas.com | sudo sh - name : Pull custom templates from stack.yml run : faas-cli template pull stack - name : Set up QEMU uses : docker/setup-qemu-action@v1 - name : Set up Docker Buildx uses : docker/setup-buildx-action@v1 - name : Get TAG id : get_tag run : echo ::set-output name=TAG::latest-dev - name : Get Repo Owner id : get_repo_owner run : > echo ::set-output name=repo_owner::$(echo ${{ github.repository_owner }} | tr '[:upper:]' '[:lower:]') - name : Docker Login run : > echo ${{secrets.GITHUB_TOKEN}} | docker login ghcr.io --username ${{ steps.get_repo_owner.outputs.repo_owner }} --password-stdin - name : Publish functions run : > OWNER=\"${{ steps.get_repo_owner.outputs.repo_owner }}\" TAG=\"latest\" faas-cli publish --extra-tag ${{ github.sha }} --build-arg GO111MODULE=on --platforms linux/amd64,linux/arm64,linux/arm/v7 --filter bcrypt If you'd like to deploy the function, check out a more comprehensive example of how to log in and deploy in Serverless For Everyone Else","title":"Publish an OpenFaaS function"},{"location":"examples/openfaas-publish/#example-publish-an-openfaas-function","text":"This example will publish an OpenFaaS function to GitHub's Container Registry (GHCR). The example uses Docker's buildx and QEMU for a multi-arch build Dynamic variables to inject the SHA and OWNER name from the repo Uses the token that GitHub assigns to the action to publish the containers. You can also run this example on GitHub's own hosted runners. Docker CE is preinstalled in the actuated VM image, and will start upon boot-up. Certified for: x86_64 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ .","title":"Example: Publish an OpenFaaS function"},{"location":"examples/openfaas-publish/#try-out-the-action-on-your-agent","text":"For alexellis' repository called alexellis/autoscaling-functions , then check out the .github/workspaces/publish.yml file: The \"Setup QEMU\" and \"Set up Docker Buildx\" steps configure the builder to produce a multi-arch image. The \"OWNER\" variable means this action can be run on any organisation without having to hard-code a username for GHCR. Only the bcrypt function is being built with the --filter command added, remove it to build all functions in the stack.yml. --platforms linux/amd64,linux/arm64,linux/arm/v7 will build for regular Intel/AMD machines, 64-bit ARM and 32-bit ARM i.e. Raspberry Pi, most users can reduce this list to just \"linux/amd64\" for a speed improvement Make sure you edit runs-on: and set it to runs-on: actuated name : publish on : push : branches : - '*' pull_request : branches : - '*' permissions : actions : read checks : write contents : read packages : write jobs : publish : runs-on : actuated steps : - uses : actions/checkout@master with : fetch-depth : 1 - name : Get faas-cli run : curl -sLSf https://cli.openfaas.com | sudo sh - name : Pull custom templates from stack.yml run : faas-cli template pull stack - name : Set up QEMU uses : docker/setup-qemu-action@v1 - name : Set up Docker Buildx uses : docker/setup-buildx-action@v1 - name : Get TAG id : get_tag run : echo ::set-output name=TAG::latest-dev - name : Get Repo Owner id : get_repo_owner run : > echo ::set-output name=repo_owner::$(echo ${{ github.repository_owner }} | tr '[:upper:]' '[:lower:]') - name : Docker Login run : > echo ${{secrets.GITHUB_TOKEN}} | docker login ghcr.io --username ${{ steps.get_repo_owner.outputs.repo_owner }} --password-stdin - name : Publish functions run : > OWNER=\"${{ steps.get_repo_owner.outputs.repo_owner }}\" TAG=\"latest\" faas-cli publish --extra-tag ${{ github.sha }} --build-arg GO111MODULE=on --platforms linux/amd64,linux/arm64,linux/arm/v7 --filter bcrypt If you'd like to deploy the function, check out a more comprehensive example of how to log in and deploy in Serverless For Everyone Else","title":"Try out the action on your agent"},{"location":"examples/system-info/","text":"Example: Get system information about your microVM \u00b6 This sample reveals system information about your runner. Certified for: x86_64 arm64 including Raspberry Pi 4 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ . Try out the action on your agent \u00b6 Create a specs.sh file: #!/bin/bash hostname whoami echo Information on main disk df -h / echo Memory info free -h echo Total CPUs: echo CPUs: nproc echo CPU Model cat /proc/cpuinfo | grep \"model name\" echo Kernel and OS info uname -a cat /etc/os-release echo PATH defined as: echo $PATH echo Egress IP defined as: curl -sLS https://checkip.amazonaws.com Create a new file at: .github/workspaces/build.yml and commit it to the repository. name : CI on : pull_request : branches : - '*' push : branches : - master jobs : specs : name : specs runs-on : actuated steps : - uses : actions/checkout@v1 - name : Check specs run : | ./specs.sh Note how the hostname changes every time the job is run.","title":"System Info"},{"location":"examples/system-info/#example-get-system-information-about-your-microvm","text":"This sample reveals system information about your runner. Certified for: x86_64 arm64 including Raspberry Pi 4 Use a private repository GitHub recommends using a private repository with self-hosted runners. Learn why in the FAQ .","title":"Example: Get system information about your microVM"},{"location":"examples/system-info/#try-out-the-action-on-your-agent","text":"Create a specs.sh file: #!/bin/bash hostname whoami echo Information on main disk df -h / echo Memory info free -h echo Total CPUs: echo CPUs: nproc echo CPU Model cat /proc/cpuinfo | grep \"model name\" echo Kernel and OS info uname -a cat /etc/os-release echo PATH defined as: echo $PATH echo Egress IP defined as: curl -sLS https://checkip.amazonaws.com Create a new file at: .github/workspaces/build.yml and commit it to the repository. name : CI on : pull_request : branches : - '*' push : branches : - master jobs : specs : name : specs runs-on : actuated steps : - uses : actions/checkout@v1 - name : Check specs run : | ./specs.sh Note how the hostname changes every time the job is run.","title":"Try out the action on your agent"}]}